# llm学习-大规模预训练语言模型

LLM的学习笔记

参考https://www.bilibili.com/video/BV1UG411p7zv

> 数据准备
> 模型构建
> 训练策略
> 模型评估与改进
> 安全、隐私、环境和法律道德问题

适应adaptation：

输入：
- 任务的自然语言描述
- 一组训练实例

两种方式：
- 训练（标准的有监督学习）：训练一个新模型，使其能将输入映射到输出。创建新模型并利用语言模型作为特征，或从现有语言模型出发，根据训练实例进行微调，或在二者之间找到平衡。
- 提示（上下文）学习：根据对任务的描述创建一个/一组提示/上下文信息，输入到语言模型中获取基于该任务的生成结果。
  - 零样本学习(Zero-shot)：提示/上下文信息的数量为0，模型直接基于对任务的理解输出结果。
  - 单样本学习(One-shot)：提示/上下文信息的数量为1，一般来说模型基于1个例子可以更好的理解任务从而较好的生成结果。
  - 少样本学习(Few-shot)：提示/上下文信息的数量大于1，大模型可以看到更丰富的例子，一般来说获得比单样本学习更好的效果。

选择过程中可能会因为过拟合变得有挑战性，如何进行有效训练？

提示的局限性在于“只能用少量的训练实例”，这种局限性由于Transformer自身的局限性导致，模型可输入的长度具有约束（一般是2048个tokens）。

GPT-3在大量任务上的表现
- 定义：任务是什么？动机？
- 适应：如何通过提示将任务简化为语言模型？
- 结果：与该任务的最先进模型相比，定量性能如何？